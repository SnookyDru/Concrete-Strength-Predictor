{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7672063,"sourceType":"datasetVersion","datasetId":4474990}],"dockerImageVersionId":30664,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch #to import pytorch\nimport torch.nn as nn #to import the neural network library as nn\nimport torch.optim as optim #to import the optimisation library as optim\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset # data loader importing\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-03-08T16:59:25.434033Z","iopub.execute_input":"2024-03-08T16:59:25.434450Z","iopub.status.idle":"2024-03-08T16:59:25.442278Z","shell.execute_reply.started":"2024-03-08T16:59:25.434420Z","shell.execute_reply":"2024-03-08T16:59:25.440568Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Load the dataset\npath = os.path.join(\"/kaggle/input/concretedataset/Concrete_Data.csv\")\nconcrete_data = pd.read_csv(path)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T16:58:30.224081Z","iopub.execute_input":"2024-03-08T16:58:30.224549Z","iopub.status.idle":"2024-03-08T16:58:30.255193Z","shell.execute_reply.started":"2024-03-08T16:58:30.224512Z","shell.execute_reply":"2024-03-08T16:58:30.254243Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"concrete_data","metadata":{"execution":{"iopub.status.busy":"2024-03-08T16:58:54.728676Z","iopub.execute_input":"2024-03-08T16:58:54.729154Z","iopub.status.idle":"2024-03-08T16:58:54.771124Z","shell.execute_reply.started":"2024-03-08T16:58:54.729118Z","shell.execute_reply":"2024-03-08T16:58:54.769586Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"      cement   slag  flyash  water  superplas  coarse_agg  fine_agg  age  \\\n0      540.0    0.0     0.0  162.0        2.5      1040.0     676.0   28   \n1      540.0    0.0     0.0  162.0        2.5      1055.0     676.0   28   \n2      332.5  142.5     0.0  228.0        0.0       932.0     594.0  270   \n3      332.5  142.5     0.0  228.0        0.0       932.0     594.0  365   \n4      198.6  132.4     0.0  192.0        0.0       978.4     825.5  360   \n...      ...    ...     ...    ...        ...         ...       ...  ...   \n1025   276.4  116.0    90.3  179.6        8.9       870.1     768.3   28   \n1026   322.2    0.0   115.6  196.0       10.4       817.9     813.4   28   \n1027   148.5  139.4   108.6  192.7        6.1       892.4     780.0   28   \n1028   159.1  186.7     0.0  175.6       11.3       989.6     788.9   28   \n1029   260.9  100.5    78.3  200.6        8.6       864.5     761.5   28   \n\n      strength  \n0        79.99  \n1        61.89  \n2        40.27  \n3        41.05  \n4        44.30  \n...        ...  \n1025     44.28  \n1026     31.18  \n1027     23.70  \n1028     32.77  \n1029     32.40  \n\n[1030 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cement</th>\n      <th>slag</th>\n      <th>flyash</th>\n      <th>water</th>\n      <th>superplas</th>\n      <th>coarse_agg</th>\n      <th>fine_agg</th>\n      <th>age</th>\n      <th>strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>79.99</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>61.89</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n      <td>40.27</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n      <td>41.05</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n      <td>44.30</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1025</th>\n      <td>276.4</td>\n      <td>116.0</td>\n      <td>90.3</td>\n      <td>179.6</td>\n      <td>8.9</td>\n      <td>870.1</td>\n      <td>768.3</td>\n      <td>28</td>\n      <td>44.28</td>\n    </tr>\n    <tr>\n      <th>1026</th>\n      <td>322.2</td>\n      <td>0.0</td>\n      <td>115.6</td>\n      <td>196.0</td>\n      <td>10.4</td>\n      <td>817.9</td>\n      <td>813.4</td>\n      <td>28</td>\n      <td>31.18</td>\n    </tr>\n    <tr>\n      <th>1027</th>\n      <td>148.5</td>\n      <td>139.4</td>\n      <td>108.6</td>\n      <td>192.7</td>\n      <td>6.1</td>\n      <td>892.4</td>\n      <td>780.0</td>\n      <td>28</td>\n      <td>23.70</td>\n    </tr>\n    <tr>\n      <th>1028</th>\n      <td>159.1</td>\n      <td>186.7</td>\n      <td>0.0</td>\n      <td>175.6</td>\n      <td>11.3</td>\n      <td>989.6</td>\n      <td>788.9</td>\n      <td>28</td>\n      <td>32.77</td>\n    </tr>\n    <tr>\n      <th>1029</th>\n      <td>260.9</td>\n      <td>100.5</td>\n      <td>78.3</td>\n      <td>200.6</td>\n      <td>8.6</td>\n      <td>864.5</td>\n      <td>761.5</td>\n      <td>28</td>\n      <td>32.40</td>\n    </tr>\n  </tbody>\n</table>\n<p>1030 rows Ã— 9 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Separate features and target\nX = concrete_data.drop(columns=['strength'])\ny = concrete_data[['strength']]","metadata":{"execution":{"iopub.status.busy":"2024-03-08T17:03:16.800392Z","iopub.execute_input":"2024-03-08T17:03:16.800836Z","iopub.status.idle":"2024-03-08T17:03:16.811801Z","shell.execute_reply.started":"2024-03-08T17:03:16.800807Z","shell.execute_reply":"2024-03-08T17:03:16.810530Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T17:06:52.703611Z","iopub.execute_input":"2024-03-08T17:06:52.704092Z","iopub.status.idle":"2024-03-08T17:06:52.713903Z","shell.execute_reply.started":"2024-03-08T17:06:52.704044Z","shell.execute_reply":"2024-03-08T17:06:52.711625Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Standardize features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T17:10:30.830084Z","iopub.execute_input":"2024-03-08T17:10:30.830604Z","iopub.status.idle":"2024-03-08T17:10:30.846705Z","shell.execute_reply.started":"2024-03-08T17:10:30.830571Z","shell.execute_reply":"2024-03-08T17:10:30.845058Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2024-03-08T17:10:40.554679Z","iopub.execute_input":"2024-03-08T17:10:40.555182Z","iopub.status.idle":"2024-03-08T17:10:40.565333Z","shell.execute_reply.started":"2024-03-08T17:10:40.555145Z","shell.execute_reply":"2024-03-08T17:10:40.563799Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"array([[-1.16087734,  0.85738747,  0.9824489 , ..., -0.25746155,\n        -0.64752011, -0.27567315],\n       [ 1.30862304, -0.60249189,  1.23259821, ..., -1.92694961,\n        -0.2731482 , -0.27567315],\n       [-0.0768653 , -0.85558366,  1.06687429, ...,  1.01785948,\n         0.06662828, -0.68931339],\n       ...,\n       [-0.86591441, -0.85558366,  1.12628475, ...,  1.34082214,\n         0.33103616,  0.91561074],\n       [ 1.78316909,  0.51111191, -0.83113361, ..., -1.54422615,\n         0.11605031, -0.27567315],\n       [ 0.28509237, -0.85558366,  0.9355459 , ..., -0.6172979 ,\n         0.13458358, -0.27567315]])"},"metadata":{}}]},{"cell_type":"code","source":"# Convert data into PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T18:22:13.437300Z","iopub.execute_input":"2024-03-08T18:22:13.438624Z","iopub.status.idle":"2024-03-08T18:22:13.446280Z","shell.execute_reply.started":"2024-03-08T18:22:13.438582Z","shell.execute_reply":"2024-03-08T18:22:13.444991Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"X_train_tensor.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-08T18:22:13.830558Z","iopub.execute_input":"2024-03-08T18:22:13.831012Z","iopub.status.idle":"2024-03-08T18:22:13.838624Z","shell.execute_reply.started":"2024-03-08T18:22:13.830978Z","shell.execute_reply":"2024-03-08T18:22:13.837238Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"torch.Size([824, 8])"},"metadata":{}}]},{"cell_type":"code","source":"# Create DataLoader\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T18:23:42.030923Z","iopub.execute_input":"2024-03-08T18:23:42.031434Z","iopub.status.idle":"2024-03-08T18:23:42.038162Z","shell.execute_reply.started":"2024-03-08T18:23:42.031399Z","shell.execute_reply":"2024-03-08T18:23:42.036573Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# Define the ANN model\nclass ANN(nn.Module):\n    def __init__(self, input_dim):\n        super(ANN, self).__init__()\n        self.input_layer = nn.Linear(input_dim, 64)\n        self.hidden_layer = nn.Linear(64, 32)\n        self.hidden_layer2 = nn.Linear(32, 8)\n        self.output_layer = nn.Linear(8, 1)  # Output layer with 1 neurons for 1 outputs\n\n    def forward(self, x):\n        x = torch.relu(self.input_layer(x))\n        x = torch.relu(self.hidden_layer(x))\n        x = torch.relu(self.hidden_layer2(x))\n        x = self.output_layer(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-03-08T18:23:42.610014Z","iopub.execute_input":"2024-03-08T18:23:42.610453Z","iopub.status.idle":"2024-03-08T18:23:42.618308Z","shell.execute_reply.started":"2024-03-08T18:23:42.610423Z","shell.execute_reply":"2024-03-08T18:23:42.617384Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# Initialize the model\ninput_dim = X_train.shape[1]\nmodel = ANN(input_dim)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T18:23:43.589330Z","iopub.execute_input":"2024-03-08T18:23:43.589751Z","iopub.status.idle":"2024-03-08T18:23:43.597103Z","shell.execute_reply.started":"2024-03-08T18:23:43.589723Z","shell.execute_reply":"2024-03-08T18:23:43.595555Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# Define loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\n# Train the model\nnum_epochs = 500\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0\n    for inputs, targets in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader)}')","metadata":{"execution":{"iopub.status.busy":"2024-03-08T18:23:44.788461Z","iopub.execute_input":"2024-03-08T18:23:44.789146Z","iopub.status.idle":"2024-03-08T18:23:55.646247Z","shell.execute_reply.started":"2024-03-08T18:23:44.789114Z","shell.execute_reply":"2024-03-08T18:23:55.645100Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Epoch [1/500], Loss: 1515.3446044921875\nEpoch [2/500], Loss: 743.1970930833083\nEpoch [3/500], Loss: 185.35461953970102\nEpoch [4/500], Loss: 139.2203914935772\nEpoch [5/500], Loss: 132.0504860511193\nEpoch [6/500], Loss: 117.04020045353816\nEpoch [7/500], Loss: 100.02680793175331\nEpoch [8/500], Loss: 98.4258058988131\nEpoch [9/500], Loss: 141.41987492487982\nEpoch [10/500], Loss: 79.03017513568585\nEpoch [11/500], Loss: 93.36850533118614\nEpoch [12/500], Loss: 85.49883915827824\nEpoch [13/500], Loss: 97.40493803757887\nEpoch [14/500], Loss: 96.53049615713266\nEpoch [15/500], Loss: 93.54944581251878\nEpoch [16/500], Loss: 57.25856927724985\nEpoch [17/500], Loss: 59.56708497267503\nEpoch [18/500], Loss: 58.430218329796425\nEpoch [19/500], Loss: 101.81065221933218\nEpoch [20/500], Loss: 93.46622789823093\nEpoch [21/500], Loss: 53.95529556274414\nEpoch [22/500], Loss: 48.20502589299129\nEpoch [23/500], Loss: 90.46743158193735\nEpoch [24/500], Loss: 52.975609852717476\nEpoch [25/500], Loss: 44.24969438406137\nEpoch [26/500], Loss: 42.42235873295711\nEpoch [27/500], Loss: 43.56383690467248\nEpoch [28/500], Loss: 77.78578259394719\nEpoch [29/500], Loss: 67.14440888624925\nEpoch [30/500], Loss: 61.02127940838154\nEpoch [31/500], Loss: 41.75755999638484\nEpoch [32/500], Loss: 37.33676881056566\nEpoch [33/500], Loss: 57.47228152935322\nEpoch [34/500], Loss: 41.956417964054985\nEpoch [35/500], Loss: 47.186333436232346\nEpoch [36/500], Loss: 50.589852993304916\nEpoch [37/500], Loss: 69.46105810312125\nEpoch [38/500], Loss: 79.88419547447792\nEpoch [39/500], Loss: 57.9877803509052\nEpoch [40/500], Loss: 57.89045715332031\nEpoch [41/500], Loss: 36.70944815415602\nEpoch [42/500], Loss: 40.73789845980131\nEpoch [43/500], Loss: 37.846156927255485\nEpoch [44/500], Loss: 38.44835706857535\nEpoch [45/500], Loss: 37.455306419959435\nEpoch [46/500], Loss: 38.007377477792595\nEpoch [47/500], Loss: 29.65732163649339\nEpoch [48/500], Loss: 49.66465861980732\nEpoch [49/500], Loss: 47.66373106149527\nEpoch [50/500], Loss: 47.78553713285006\nEpoch [51/500], Loss: 36.736133135282074\nEpoch [52/500], Loss: 33.08410673875075\nEpoch [53/500], Loss: 38.11728382110596\nEpoch [54/500], Loss: 51.62867678128756\nEpoch [55/500], Loss: 31.661453393789436\nEpoch [56/500], Loss: 70.99405831557054\nEpoch [57/500], Loss: 30.05834594139686\nEpoch [58/500], Loss: 26.569439227764423\nEpoch [59/500], Loss: 43.401596362774185\nEpoch [60/500], Loss: 28.329077060406025\nEpoch [61/500], Loss: 30.66485199561486\nEpoch [62/500], Loss: 39.993172278771034\nEpoch [63/500], Loss: 56.012732139000526\nEpoch [64/500], Loss: 32.178065666785606\nEpoch [65/500], Loss: 30.899214817927433\nEpoch [66/500], Loss: 30.253722557654747\nEpoch [67/500], Loss: 35.778619472797104\nEpoch [68/500], Loss: 31.04863460247333\nEpoch [69/500], Loss: 38.64477509718675\nEpoch [70/500], Loss: 28.49156218308669\nEpoch [71/500], Loss: 23.925205817589394\nEpoch [72/500], Loss: 26.09945796086238\nEpoch [73/500], Loss: 51.8087714268611\nEpoch [74/500], Loss: 25.22434586745042\nEpoch [75/500], Loss: 24.008242973914513\nEpoch [76/500], Loss: 29.994044523972732\nEpoch [77/500], Loss: 53.62162472651555\nEpoch [78/500], Loss: 34.18730838482197\nEpoch [79/500], Loss: 26.79481785113995\nEpoch [80/500], Loss: 30.90425550020658\nEpoch [81/500], Loss: 25.54219260582557\nEpoch [82/500], Loss: 30.18819412818322\nEpoch [83/500], Loss: 37.021816106942985\nEpoch [84/500], Loss: 22.396938470693733\nEpoch [85/500], Loss: 35.37436074476976\nEpoch [86/500], Loss: 24.979464824383076\nEpoch [87/500], Loss: 33.41327359126164\nEpoch [88/500], Loss: 25.39794210287241\nEpoch [89/500], Loss: 39.62911693866436\nEpoch [90/500], Loss: 44.97347068786621\nEpoch [91/500], Loss: 28.37482239649846\nEpoch [92/500], Loss: 24.994505368746243\nEpoch [93/500], Loss: 24.18306387387789\nEpoch [94/500], Loss: 45.17865540431096\nEpoch [95/500], Loss: 21.986922704256497\nEpoch [96/500], Loss: 20.15868245638334\nEpoch [97/500], Loss: 20.50148817209097\nEpoch [98/500], Loss: 28.54712515610915\nEpoch [99/500], Loss: 43.50568786034217\nEpoch [100/500], Loss: 33.88868522644043\nEpoch [101/500], Loss: 19.584657228910007\nEpoch [102/500], Loss: 21.690191415640022\nEpoch [103/500], Loss: 48.35687314547025\nEpoch [104/500], Loss: 20.485852901752178\nEpoch [105/500], Loss: 20.974611062269943\nEpoch [106/500], Loss: 23.281565519479607\nEpoch [107/500], Loss: 22.487269474909855\nEpoch [108/500], Loss: 21.448999404907227\nEpoch [109/500], Loss: 20.349765043992264\nEpoch [110/500], Loss: 19.433954825768105\nEpoch [111/500], Loss: 19.50277064396785\nEpoch [112/500], Loss: 47.24374228257399\nEpoch [113/500], Loss: 21.161651317889874\nEpoch [114/500], Loss: 19.736481813284065\nEpoch [115/500], Loss: 25.90289600078876\nEpoch [116/500], Loss: 27.99607174213116\nEpoch [117/500], Loss: 18.713667135972244\nEpoch [118/500], Loss: 24.13232759328989\nEpoch [119/500], Loss: 26.260819288400505\nEpoch [120/500], Loss: 20.782496525691105\nEpoch [121/500], Loss: 22.3914972452017\nEpoch [122/500], Loss: 41.64603512103741\nEpoch [123/500], Loss: 21.79771592066838\nEpoch [124/500], Loss: 23.230614222013035\nEpoch [125/500], Loss: 26.11238817068247\nEpoch [126/500], Loss: 29.6941809287438\nEpoch [127/500], Loss: 22.51624276087834\nEpoch [128/500], Loss: 22.974888141338642\nEpoch [129/500], Loss: 41.87176924485426\nEpoch [130/500], Loss: 17.871454605689415\nEpoch [131/500], Loss: 23.256873570955715\nEpoch [132/500], Loss: 18.841425382173977\nEpoch [133/500], Loss: 15.939116111168495\nEpoch [134/500], Loss: 21.489401597243088\nEpoch [135/500], Loss: 24.40894258939303\nEpoch [136/500], Loss: 30.11778024526743\nEpoch [137/500], Loss: 28.894748541025017\nEpoch [138/500], Loss: 20.852693997896633\nEpoch [139/500], Loss: 20.184032146747295\nEpoch [140/500], Loss: 31.70237027681791\nEpoch [141/500], Loss: 35.44531059265137\nEpoch [142/500], Loss: 23.951352779681866\nEpoch [143/500], Loss: 19.77189093369704\nEpoch [144/500], Loss: 17.04163698049692\nEpoch [145/500], Loss: 26.807098388671875\nEpoch [146/500], Loss: 24.178213192866398\nEpoch [147/500], Loss: 22.835430145263672\nEpoch [148/500], Loss: 21.362938954279972\nEpoch [149/500], Loss: 17.864766487708458\nEpoch [150/500], Loss: 16.616015947782078\nEpoch [151/500], Loss: 16.926091854388897\nEpoch [152/500], Loss: 18.87247107579158\nEpoch [153/500], Loss: 16.092631853543796\nEpoch [154/500], Loss: 19.858988395104042\nEpoch [155/500], Loss: 30.47815124805157\nEpoch [156/500], Loss: 21.031339938823994\nEpoch [157/500], Loss: 29.78665073101337\nEpoch [158/500], Loss: 17.24054087125338\nEpoch [159/500], Loss: 25.398478141197792\nEpoch [160/500], Loss: 20.530990013709435\nEpoch [161/500], Loss: 25.28194097372202\nEpoch [162/500], Loss: 18.070740186251125\nEpoch [163/500], Loss: 25.530953187208908\nEpoch [164/500], Loss: 18.815225967994103\nEpoch [165/500], Loss: 21.315057094280537\nEpoch [166/500], Loss: 18.297848554757927\nEpoch [167/500], Loss: 36.78356478764461\nEpoch [168/500], Loss: 18.217948033259464\nEpoch [169/500], Loss: 22.51953565157377\nEpoch [170/500], Loss: 15.396159172058105\nEpoch [171/500], Loss: 18.94429243527926\nEpoch [172/500], Loss: 25.58518439072829\nEpoch [173/500], Loss: 36.60149163466234\nEpoch [174/500], Loss: 17.476755655728855\nEpoch [175/500], Loss: 15.659007732684795\nEpoch [176/500], Loss: 16.755492797264687\nEpoch [177/500], Loss: 18.243353843688965\nEpoch [178/500], Loss: 14.554261867816631\nEpoch [179/500], Loss: 17.444657692542442\nEpoch [180/500], Loss: 17.157082557678223\nEpoch [181/500], Loss: 24.711565017700195\nEpoch [182/500], Loss: 28.849767831655647\nEpoch [183/500], Loss: 22.608920464148888\nEpoch [184/500], Loss: 15.370248794555664\nEpoch [185/500], Loss: 18.624536000765286\nEpoch [186/500], Loss: 18.394584362323467\nEpoch [187/500], Loss: 23.949140475346493\nEpoch [188/500], Loss: 29.963930276724007\nEpoch [189/500], Loss: 18.00436570094182\nEpoch [190/500], Loss: 24.692968075092022\nEpoch [191/500], Loss: 22.844484475942757\nEpoch [192/500], Loss: 16.007320697490986\nEpoch [193/500], Loss: 33.68338218102088\nEpoch [194/500], Loss: 14.129033455481895\nEpoch [195/500], Loss: 17.24418720832238\nEpoch [196/500], Loss: 18.326500159043533\nEpoch [197/500], Loss: 15.212525367736816\nEpoch [198/500], Loss: 19.868487431452824\nEpoch [199/500], Loss: 14.694581545316256\nEpoch [200/500], Loss: 20.414181782649113\nEpoch [201/500], Loss: 15.434550725496733\nEpoch [202/500], Loss: 26.465021646939793\nEpoch [203/500], Loss: 16.16171231636634\nEpoch [204/500], Loss: 22.760867852431076\nEpoch [205/500], Loss: 24.86713585486779\nEpoch [206/500], Loss: 13.547373624948355\nEpoch [207/500], Loss: 13.547694939833422\nEpoch [208/500], Loss: 25.069103974562424\nEpoch [209/500], Loss: 15.655562180739183\nEpoch [210/500], Loss: 22.980753825261043\nEpoch [211/500], Loss: 16.263178752018856\nEpoch [212/500], Loss: 16.871153391324558\nEpoch [213/500], Loss: 13.543001138246977\nEpoch [214/500], Loss: 13.626385835500864\nEpoch [215/500], Loss: 22.808800037090595\nEpoch [216/500], Loss: 32.5149077635545\nEpoch [217/500], Loss: 22.442481847909782\nEpoch [218/500], Loss: 19.414852509131798\nEpoch [219/500], Loss: 22.437680611243614\nEpoch [220/500], Loss: 15.139897713294395\nEpoch [221/500], Loss: 15.899819520803598\nEpoch [222/500], Loss: 14.821304027850811\nEpoch [223/500], Loss: 14.354990812448355\nEpoch [224/500], Loss: 15.794203244722807\nEpoch [225/500], Loss: 27.999717859121468\nEpoch [226/500], Loss: 14.443555758549618\nEpoch [227/500], Loss: 13.178158026475172\nEpoch [228/500], Loss: 14.201834165132963\nEpoch [229/500], Loss: 15.067054748535156\nEpoch [230/500], Loss: 16.151722321143517\nEpoch [231/500], Loss: 19.05862338726337\nEpoch [232/500], Loss: 22.76735782623291\nEpoch [233/500], Loss: 17.00853311098539\nEpoch [234/500], Loss: 13.012210405789888\nEpoch [235/500], Loss: 12.344537771665133\nEpoch [236/500], Loss: 18.912216259883\nEpoch [237/500], Loss: 17.725166284121\nEpoch [238/500], Loss: 15.52641773223877\nEpoch [239/500], Loss: 27.67313443697416\nEpoch [240/500], Loss: 16.85470746113704\nEpoch [241/500], Loss: 14.209273705115685\nEpoch [242/500], Loss: 26.024950614342323\nEpoch [243/500], Loss: 12.949844873868502\nEpoch [244/500], Loss: 15.78923984674307\nEpoch [245/500], Loss: 14.312515625586876\nEpoch [246/500], Loss: 19.275063514709473\nEpoch [247/500], Loss: 19.333981660696175\nEpoch [248/500], Loss: 12.542256648723896\nEpoch [249/500], Loss: 25.87566742530236\nEpoch [250/500], Loss: 19.828034474299503\nEpoch [251/500], Loss: 22.242439343379093\nEpoch [252/500], Loss: 19.611257333021896\nEpoch [253/500], Loss: 12.33856722024771\nEpoch [254/500], Loss: 11.971228012671837\nEpoch [255/500], Loss: 20.478888731736404\nEpoch [256/500], Loss: 19.175013908973106\nEpoch [257/500], Loss: 17.41649451622596\nEpoch [258/500], Loss: 14.238082812382626\nEpoch [259/500], Loss: 17.426637869614822\nEpoch [260/500], Loss: 15.22247468508207\nEpoch [261/500], Loss: 13.916908190800594\nEpoch [262/500], Loss: 18.845968283139744\nEpoch [263/500], Loss: 12.63923534980187\nEpoch [264/500], Loss: 15.445273436032808\nEpoch [265/500], Loss: 13.0261094386761\nEpoch [266/500], Loss: 13.01193959896381\nEpoch [267/500], Loss: 15.88750905257005\nEpoch [268/500], Loss: 11.81992483139038\nEpoch [269/500], Loss: 21.79282107720008\nEpoch [270/500], Loss: 19.347896282489483\nEpoch [271/500], Loss: 19.837646484375\nEpoch [272/500], Loss: 26.5524383691641\nEpoch [273/500], Loss: 15.61595557286189\nEpoch [274/500], Loss: 11.49917276089008\nEpoch [275/500], Loss: 12.774022102355957\nEpoch [276/500], Loss: 13.828627659724308\nEpoch [277/500], Loss: 14.470546135535606\nEpoch [278/500], Loss: 12.428467493790846\nEpoch [279/500], Loss: 11.677595872145433\nEpoch [280/500], Loss: 12.97167077431312\nEpoch [281/500], Loss: 24.627594434298\nEpoch [282/500], Loss: 12.092853986299955\nEpoch [283/500], Loss: 14.815370596372164\nEpoch [284/500], Loss: 15.981122457064115\nEpoch [285/500], Loss: 11.82797696040227\nEpoch [286/500], Loss: 11.822785524221567\nEpoch [287/500], Loss: 15.668495251582218\nEpoch [288/500], Loss: 16.009169211754433\nEpoch [289/500], Loss: 13.98593649497399\nEpoch [290/500], Loss: 24.919251075157753\nEpoch [291/500], Loss: 16.37185236123892\nEpoch [292/500], Loss: 11.921362179976244\nEpoch [293/500], Loss: 11.572095724252554\nEpoch [294/500], Loss: 12.50878825554481\nEpoch [295/500], Loss: 11.880653491387001\nEpoch [296/500], Loss: 16.222482681274414\nEpoch [297/500], Loss: 14.52469095816979\nEpoch [298/500], Loss: 11.872295526357798\nEpoch [299/500], Loss: 12.317589613107534\nEpoch [300/500], Loss: 13.960577671344463\nEpoch [301/500], Loss: 13.02062232677753\nEpoch [302/500], Loss: 14.71801970555232\nEpoch [303/500], Loss: 12.109010182894194\nEpoch [304/500], Loss: 11.184476705697866\nEpoch [305/500], Loss: 12.396841489351713\nEpoch [306/500], Loss: 10.987298341897818\nEpoch [307/500], Loss: 20.34851250281701\nEpoch [308/500], Loss: 11.588820164020245\nEpoch [309/500], Loss: 11.125086784362793\nEpoch [310/500], Loss: 22.218071387364315\nEpoch [311/500], Loss: 15.9470765040471\nEpoch [312/500], Loss: 11.193444912250225\nEpoch [313/500], Loss: 26.408629564138558\nEpoch [314/500], Loss: 12.919601953946627\nEpoch [315/500], Loss: 11.118083440340483\nEpoch [316/500], Loss: 15.265479014470028\nEpoch [317/500], Loss: 10.658790845137377\nEpoch [318/500], Loss: 10.836087043468769\nEpoch [319/500], Loss: 12.031308724330021\nEpoch [320/500], Loss: 11.063679951887865\nEpoch [321/500], Loss: 13.346020551828238\nEpoch [322/500], Loss: 30.01664315737211\nEpoch [323/500], Loss: 11.793735797588642\nEpoch [324/500], Loss: 15.184010945833647\nEpoch [325/500], Loss: 17.145966896644005\nEpoch [326/500], Loss: 10.340879880464994\nEpoch [327/500], Loss: 11.06313940194937\nEpoch [328/500], Loss: 19.831461979792667\nEpoch [329/500], Loss: 19.955261303828312\nEpoch [330/500], Loss: 12.108379400693453\nEpoch [331/500], Loss: 14.270400047302246\nEpoch [332/500], Loss: 11.349492109738863\nEpoch [333/500], Loss: 11.186783900627724\nEpoch [334/500], Loss: 18.604097586411697\nEpoch [335/500], Loss: 24.574268487783577\nEpoch [336/500], Loss: 10.885236299954927\nEpoch [337/500], Loss: 10.566198459038368\nEpoch [338/500], Loss: 14.69642895918626\nEpoch [339/500], Loss: 16.140668062063362\nEpoch [340/500], Loss: 10.346690251277042\nEpoch [341/500], Loss: 13.344901158259464\nEpoch [342/500], Loss: 10.591612375699556\nEpoch [343/500], Loss: 12.248574990492601\nEpoch [344/500], Loss: 12.588894330538237\nEpoch [345/500], Loss: 13.138480589939999\nEpoch [346/500], Loss: 23.690971814669094\nEpoch [347/500], Loss: 12.889883334820087\nEpoch [348/500], Loss: 17.571904989389274\nEpoch [349/500], Loss: 13.278221974006065\nEpoch [350/500], Loss: 18.413606130159817\nEpoch [351/500], Loss: 13.893165881817158\nEpoch [352/500], Loss: 10.93721947303185\nEpoch [353/500], Loss: 11.612897065969614\nEpoch [354/500], Loss: 10.77712843968318\nEpoch [355/500], Loss: 10.671862235436073\nEpoch [356/500], Loss: 13.890223796551044\nEpoch [357/500], Loss: 15.11317935356727\nEpoch [358/500], Loss: 23.90931334862342\nEpoch [359/500], Loss: 15.471557470468374\nEpoch [360/500], Loss: 16.378258264981785\nEpoch [361/500], Loss: 12.88598973934467\nEpoch [362/500], Loss: 14.58904688174908\nEpoch [363/500], Loss: 10.154151806464562\nEpoch [364/500], Loss: 21.733234258798454\nEpoch [365/500], Loss: 11.769843028141903\nEpoch [366/500], Loss: 15.930457188532902\nEpoch [367/500], Loss: 14.152294709132267\nEpoch [368/500], Loss: 10.818204219524677\nEpoch [369/500], Loss: 12.430919940655048\nEpoch [370/500], Loss: 12.155508004702055\nEpoch [371/500], Loss: 10.923548148228573\nEpoch [372/500], Loss: 14.880441078772911\nEpoch [373/500], Loss: 11.91479257436899\nEpoch [374/500], Loss: 10.543940030611479\nEpoch [375/500], Loss: 20.261022274310772\nEpoch [376/500], Loss: 12.34517526626587\nEpoch [377/500], Loss: 9.791831016540527\nEpoch [378/500], Loss: 9.849972944993238\nEpoch [379/500], Loss: 18.64266245181744\nEpoch [380/500], Loss: 18.167933464050293\nEpoch [381/500], Loss: 10.17680626649123\nEpoch [382/500], Loss: 9.666513296274038\nEpoch [383/500], Loss: 10.754981444432186\nEpoch [384/500], Loss: 17.769802533663235\nEpoch [385/500], Loss: 10.528851142296425\nEpoch [386/500], Loss: 10.22179207435021\nEpoch [387/500], Loss: 17.17763145153339\nEpoch [388/500], Loss: 18.496857276329628\nEpoch [389/500], Loss: 20.909233019902157\nEpoch [390/500], Loss: 9.779760434077335\nEpoch [391/500], Loss: 14.919022119962252\nEpoch [392/500], Loss: 11.509168844956617\nEpoch [393/500], Loss: 10.388583219968355\nEpoch [394/500], Loss: 12.321998889629658\nEpoch [395/500], Loss: 10.047808757195106\nEpoch [396/500], Loss: 9.821262799776518\nEpoch [397/500], Loss: 10.788179947779728\nEpoch [398/500], Loss: 14.436246431790865\nEpoch [399/500], Loss: 10.643758113567646\nEpoch [400/500], Loss: 11.201059964986948\nEpoch [401/500], Loss: 16.389150876265305\nEpoch [402/500], Loss: 12.698918415949894\nEpoch [403/500], Loss: 9.792458864358755\nEpoch [404/500], Loss: 21.899899042569675\nEpoch [405/500], Loss: 29.508395268366886\nEpoch [406/500], Loss: 15.486679590665377\nEpoch [407/500], Loss: 10.786303446843075\nEpoch [408/500], Loss: 16.035178734705998\nEpoch [409/500], Loss: 11.451446826641376\nEpoch [410/500], Loss: 15.816912797781137\nEpoch [411/500], Loss: 16.075422837184025\nEpoch [412/500], Loss: 12.02531836583064\nEpoch [413/500], Loss: 12.413829803466797\nEpoch [414/500], Loss: 13.52614113000723\nEpoch [415/500], Loss: 13.580443162184496\nEpoch [416/500], Loss: 12.18589760706975\nEpoch [417/500], Loss: 10.568161487579346\nEpoch [418/500], Loss: 11.702727684607872\nEpoch [419/500], Loss: 11.990222930908203\nEpoch [420/500], Loss: 15.244248243478628\nEpoch [421/500], Loss: 17.618218495295597\nEpoch [422/500], Loss: 14.936045573307918\nEpoch [423/500], Loss: 11.50598221558791\nEpoch [424/500], Loss: 9.142717361450195\nEpoch [425/500], Loss: 9.858402472275953\nEpoch [426/500], Loss: 14.342681041130653\nEpoch [427/500], Loss: 11.84107567713811\nEpoch [428/500], Loss: 10.771411345555233\nEpoch [429/500], Loss: 11.06958899131188\nEpoch [430/500], Loss: 19.3525788967426\nEpoch [431/500], Loss: 20.326860427856445\nEpoch [432/500], Loss: 12.313168965853178\nEpoch [433/500], Loss: 9.302482604980469\nEpoch [434/500], Loss: 10.294181493612436\nEpoch [435/500], Loss: 9.593933802384596\nEpoch [436/500], Loss: 9.722234725952148\nEpoch [437/500], Loss: 11.794475922217735\nEpoch [438/500], Loss: 14.268314508291391\nEpoch [439/500], Loss: 11.52833945934589\nEpoch [440/500], Loss: 11.22709765801063\nEpoch [441/500], Loss: 12.23430897639348\nEpoch [442/500], Loss: 8.8508148926955\nEpoch [443/500], Loss: 9.832924035879282\nEpoch [444/500], Loss: 14.635099961207462\nEpoch [445/500], Loss: 18.906640786391037\nEpoch [446/500], Loss: 12.666233502901518\nEpoch [447/500], Loss: 10.996757360605093\nEpoch [448/500], Loss: 20.27179054113535\nEpoch [449/500], Loss: 9.254572611588697\nEpoch [450/500], Loss: 9.014622871692364\nEpoch [451/500], Loss: 10.693257111769457\nEpoch [452/500], Loss: 19.535302859086258\nEpoch [453/500], Loss: 9.617595562568077\nEpoch [454/500], Loss: 9.577407176677998\nEpoch [455/500], Loss: 9.696798471304087\nEpoch [456/500], Loss: 10.616090664496788\nEpoch [457/500], Loss: 16.389879116645226\nEpoch [458/500], Loss: 21.65463509926429\nEpoch [459/500], Loss: 9.083650442270132\nEpoch [460/500], Loss: 11.663722074948824\nEpoch [461/500], Loss: 10.264237807347225\nEpoch [462/500], Loss: 11.438855758080116\nEpoch [463/500], Loss: 21.00904002556434\nEpoch [464/500], Loss: 18.91151890387902\nEpoch [465/500], Loss: 12.275447882138765\nEpoch [466/500], Loss: 19.002235412597656\nEpoch [467/500], Loss: 9.748531891749455\nEpoch [468/500], Loss: 12.58118724822998\nEpoch [469/500], Loss: 15.767852563124437\nEpoch [470/500], Loss: 10.76289411691519\nEpoch [471/500], Loss: 9.59487981062669\nEpoch [472/500], Loss: 9.760350264035738\nEpoch [473/500], Loss: 15.938262609335093\nEpoch [474/500], Loss: 11.375187910520113\nEpoch [475/500], Loss: 13.794962882995605\nEpoch [476/500], Loss: 10.241816777449388\nEpoch [477/500], Loss: 12.944350462693434\nEpoch [478/500], Loss: 12.100535099322979\nEpoch [479/500], Loss: 9.169787627000074\nEpoch [480/500], Loss: 12.682986644598154\nEpoch [481/500], Loss: 12.369815496297983\nEpoch [482/500], Loss: 15.03488327906682\nEpoch [483/500], Loss: 9.238867943103497\nEpoch [484/500], Loss: 23.654178069188045\nEpoch [485/500], Loss: 10.491194028120775\nEpoch [486/500], Loss: 8.941755551558275\nEpoch [487/500], Loss: 10.086960755861723\nEpoch [488/500], Loss: 9.984472898336557\nEpoch [489/500], Loss: 12.908131672785832\nEpoch [490/500], Loss: 23.702090410085823\nEpoch [491/500], Loss: 13.27122739645151\nEpoch [492/500], Loss: 9.973875669332651\nEpoch [493/500], Loss: 10.072084830357479\nEpoch [494/500], Loss: 8.974215507507324\nEpoch [495/500], Loss: 9.602804110600399\nEpoch [496/500], Loss: 8.802451940683218\nEpoch [497/500], Loss: 16.257299423217773\nEpoch [498/500], Loss: 15.618926855234\nEpoch [499/500], Loss: 13.116583090562086\nEpoch [500/500], Loss: 10.90180121935331\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.0001) #reducing the learning rate even more to get even better result\n\n# Train the model\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0\n    for inputs, targets in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader)}')","metadata":{"execution":{"iopub.status.busy":"2024-03-08T18:26:16.277798Z","iopub.execute_input":"2024-03-08T18:26:16.278232Z","iopub.status.idle":"2024-03-08T18:26:18.537037Z","shell.execute_reply.started":"2024-03-08T18:26:16.278202Z","shell.execute_reply":"2024-03-08T18:26:18.535494Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Epoch [1/100], Loss: 4.776674032211304\nEpoch [2/100], Loss: 4.739891253984892\nEpoch [3/100], Loss: 4.7446750310751105\nEpoch [4/100], Loss: 4.75280028123122\nEpoch [5/100], Loss: 4.758197344266451\nEpoch [6/100], Loss: 4.7244895329842205\nEpoch [7/100], Loss: 4.74842014679542\nEpoch [8/100], Loss: 4.728320910380437\nEpoch [9/100], Loss: 4.777014860740075\nEpoch [10/100], Loss: 4.766154802762545\nEpoch [11/100], Loss: 4.753142411892231\nEpoch [12/100], Loss: 4.778932828169602\nEpoch [13/100], Loss: 4.744372844696045\nEpoch [14/100], Loss: 4.749992792422955\nEpoch [15/100], Loss: 4.764149335714487\nEpoch [16/100], Loss: 4.762575387954712\nEpoch [17/100], Loss: 4.73381586258228\nEpoch [18/100], Loss: 4.768836388221154\nEpoch [19/100], Loss: 4.723276230005117\nEpoch [20/100], Loss: 4.799048203688401\nEpoch [21/100], Loss: 4.747911957594065\nEpoch [22/100], Loss: 4.722835485751812\nEpoch [23/100], Loss: 4.82063075212332\nEpoch [24/100], Loss: 4.762863360918486\nEpoch [25/100], Loss: 4.748732236715464\nEpoch [26/100], Loss: 4.718440055847168\nEpoch [27/100], Loss: 4.7181231792156515\nEpoch [28/100], Loss: 4.725364098182092\nEpoch [29/100], Loss: 4.732250617100642\nEpoch [30/100], Loss: 4.703401914009681\nEpoch [31/100], Loss: 4.753460352237408\nEpoch [32/100], Loss: 4.742836016875047\nEpoch [33/100], Loss: 4.761386724618765\nEpoch [34/100], Loss: 4.728046692334688\nEpoch [35/100], Loss: 4.75740607885214\nEpoch [36/100], Loss: 4.718119639616746\nEpoch [37/100], Loss: 4.744386361195491\nEpoch [38/100], Loss: 4.701908460030189\nEpoch [39/100], Loss: 4.76439224756681\nEpoch [40/100], Loss: 4.7352710870596075\nEpoch [41/100], Loss: 4.704041939515334\nEpoch [42/100], Loss: 4.7293048455164985\nEpoch [43/100], Loss: 4.712691307067871\nEpoch [44/100], Loss: 4.762988567352295\nEpoch [45/100], Loss: 4.730242692507231\nEpoch [46/100], Loss: 4.734779577988845\nEpoch [47/100], Loss: 4.712194369389461\nEpoch [48/100], Loss: 4.703039297690759\nEpoch [49/100], Loss: 4.731632269345797\nEpoch [50/100], Loss: 4.704417962294358\nEpoch [51/100], Loss: 4.730625739464393\nEpoch [52/100], Loss: 4.73500134394719\nEpoch [53/100], Loss: 4.7198204810802755\nEpoch [54/100], Loss: 4.789732896364653\nEpoch [55/100], Loss: 4.720799996302678\nEpoch [56/100], Loss: 4.698663894946758\nEpoch [57/100], Loss: 4.728166506840632\nEpoch [58/100], Loss: 4.7201169820932245\nEpoch [59/100], Loss: 4.709919315118056\nEpoch [60/100], Loss: 4.759460907716018\nEpoch [61/100], Loss: 4.705043077468872\nEpoch [62/100], Loss: 4.712821556971623\nEpoch [63/100], Loss: 4.696885384046114\nEpoch [64/100], Loss: 4.74909756733821\nEpoch [65/100], Loss: 4.677840269528902\nEpoch [66/100], Loss: 4.721258310171274\nEpoch [67/100], Loss: 4.718817234039307\nEpoch [68/100], Loss: 4.701865214567918\nEpoch [69/100], Loss: 4.727203295781062\nEpoch [70/100], Loss: 4.698097155644343\nEpoch [71/100], Loss: 4.709924569496741\nEpoch [72/100], Loss: 4.686824450126061\nEpoch [73/100], Loss: 4.705954716755794\nEpoch [74/100], Loss: 4.719548390461848\nEpoch [75/100], Loss: 4.69266651226924\nEpoch [76/100], Loss: 4.708691688684317\nEpoch [77/100], Loss: 4.681879006899321\nEpoch [78/100], Loss: 4.6870125623849725\nEpoch [79/100], Loss: 4.6912096830514765\nEpoch [80/100], Loss: 4.784914145102868\nEpoch [81/100], Loss: 4.684450112856352\nEpoch [82/100], Loss: 4.700291606096121\nEpoch [83/100], Loss: 4.693568633152888\nEpoch [84/100], Loss: 4.666005776478694\nEpoch [85/100], Loss: 4.7033655276665325\nEpoch [86/100], Loss: 4.732722465808575\nEpoch [87/100], Loss: 4.695831894874573\nEpoch [88/100], Loss: 4.764222365159255\nEpoch [89/100], Loss: 4.693257001730112\nEpoch [90/100], Loss: 4.785055582339947\nEpoch [91/100], Loss: 4.718427382982695\nEpoch [92/100], Loss: 4.696189733651968\nEpoch [93/100], Loss: 4.6999379488138056\nEpoch [94/100], Loss: 4.664758104544419\nEpoch [95/100], Loss: 4.670017535869892\nEpoch [96/100], Loss: 4.726400522085337\nEpoch [97/100], Loss: 4.689341691824106\nEpoch [98/100], Loss: 4.694903832215529\nEpoch [99/100], Loss: 4.678108765528752\nEpoch [100/100], Loss: 4.664834682758038\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the model\nmodel.eval()\nwith torch.no_grad():\n    y_pred = model(X_test_tensor)\n    test_loss = criterion(y_pred, y_test_tensor)\n    print(f'Test Loss: {test_loss.item()}')","metadata":{"execution":{"iopub.status.busy":"2024-03-08T18:26:29.228440Z","iopub.execute_input":"2024-03-08T18:26:29.228899Z","iopub.status.idle":"2024-03-08T18:26:29.236222Z","shell.execute_reply.started":"2024-03-08T18:26:29.228868Z","shell.execute_reply":"2024-03-08T18:26:29.235317Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Test Loss: 29.01835060119629\n","output_type":"stream"}]}]}